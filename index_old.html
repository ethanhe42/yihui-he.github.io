<!-- <!DOCTYPE html> -->
<html lang="en">
  <head>
    <script
      async
      src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4253216309174736"
      crossorigin="anonymous"
    ></script>
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y4TMBBH2RZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y4TMBBH2RZ');
</script>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>
      Yihui He, CMU grad student focused on Computer vision & Deep Learning
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet" />
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet" />
    <link href="./assets_files/yangqing.css" rel="stylesheet" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    <link rel="icon" href="./assets_files/favicon.ico" />
  </head>

  <div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#research">Research</a>-->
    <a href="#publications">Publications</a>
    <a href="#projects">Projects</a>
    <a href="meet.html">Coaching</a>
    <!--<a href="#teaching">Teaching</a>-->
    <a
      target="_blank"
      href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8"
      >CV</a
    >
  </div>

  <body>
    <div class="container span3 hidden-phone">
      <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br />
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
          <li><a href="#top">About</a></li>
          <!--<li><a href="#research">Research</a></li>-->
          <li><a href="#publications">Publications</a></li>
          <li><a href="#projects">Projects</a></li>
          <a href="meet.html">Coaching</a>
          <!--<li><a href="#teaching">Teaching</a></li>-->
          <!--             <li><a target="_blank"
                   href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8">CV</a>
            </li> -->
        </ul>
        <hr class="hidden-phone" />
        <div class="text-center hidden-phone">
          <img src="assets_files/me.png" alt="photo" class="logo-image" />
          <br /><br />
          yihuihe.yh AT gmail.com <br />
        </div>

        <!-- Otherwise, we simply use a flat list of links -->
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="span9">
          <br />
          <h3>Yihui He (何宜晖)</h3>
          <h5>
            yihuihe.yh AT gmail.com /
            <a target="_blank" href="https://twitter.com/he_yi_hui"
              >twitter@he_yi_hui</a
            >
            /
            <a
              target="_blank"
              href="https://www.youtube.com/channel/UCuxl5K1MSj1TTtAsNmlONcg"
              >YouTube</a
            >
            /
            <a target="_blank" href="https://github.com/yihui-he"
              >GitHub (2k followers)</a
            >
            /
            <a target="_blank" href="https://www.zhihu.com/people/rex686568"
              >知乎 (8k 关注)</a
            >
            <!--             / <a target="_blank"
                 href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8">CV</a> -->
            <!--             / <mark>actively looking for job</mark> -->
          </h5>
          <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
          <a
            class="visible-phone pull-left"
            href="http://daggerfs.com/index.html#"
          >
            <img
              class="media-object"
              src="assets_files/me.png"
              width="96px"
              style="margin: 0px 10px"
            />
          </a>
          <p>
            I'm a research engineer at
            <a target="_blank" href="https://www.goheadroom.com">Headroom</a>.
            Before that, I was a research engineer at Facebook AI Research.
          </p>
          <p>
            I got master's degree from CMU , with my interest focus on Computer
            Vision and Deep Learning. At CMU, my capstone project is on
            multi-view pose estimation, with professor
            <a target="_blank" href="https://www.cs.cmu.edu/~katef/"
              >Katerina Fragkiadaki</a
            >. I interned at Facebook AI Research mentored by
            <a
              target="_blank"
              href="https://research.fb.com/people/goyal-priya/"
              >Priya Goyal</a
            >
            and
            <a target="_blank" href="http://www.cs.cmu.edu/~abhinavg/"
              >Abhinav Gupta</a
            >.
          </p>
          <p>
            I have a track record of contributing to CNN efficient inference
            during my undergrad study. Particularly, I designed
            <a href="#cp">channel pruning</a> to effectively prune channels. I
            further proposed <a href="#AMC">AMC</a> to sample the design space
            of channel pruning via reinforcement learning, which greatly
            improved the performance.
          </p>
          <p>
            I served as a reviewer for ECCV'20, ICML'20, CVPR'20, ICLR'20,
            ICCV'19, CVPR'19, ICLR'19, NIPS'18, Pattern Recognition Letters, TIP
            and IJCV.
          </p>

          <!--
             *** Research ***
            -->
          <!--<h3>-->
          <!--<a name="research"></a> Research-->
          <!--</h3>-->
          <!--<p>-->
          <!--My current research topics include:-->
          <!--</p><ul>-->
          <!--<li> Learning better structures for image feature extraction.-->
          <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
          <!--</li><li> Making large-scale vision feasible and affordable.-->
          <!--</li></ul>-->
          <!--<p></p>-->
          <!--<p> (Most recent publications to be added) </p>-->

          <!--
             *** Publications ***
            -->
          <h3>
            <a name="publications"></a> Publication
            <a
              target="_blank"
              href="https://scholar.google.com/citations?user=2yAMJ1YAAAAJ&hl=en"
              >(latest on Google Scholar)</a
            >
          </h3>
          <!--
            <div class="media">
                <a name="pts" class="pull-left">
                    <img class="media-object" src="./assets/pts.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Prediction-Tracking-Segmentation
                     </strong><br>
                        Jianren Wang, <strong>Yihui He</strong>, Xiaobo Wang, Xinjia Yu, Xia Chen
                        <a target="_blank"
                           href="https://arxiv.org/abs/1904.03280">[arXiv]</a>
                    </p>
                    <p class="abstract-text">
                        We motivate and present feature selective anchor-free(FSAF) module, a simple and effective building block forsingle-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. 
                    </p>
                </div>
            </div>
-->
          <div class="media">
            <a name="fsaf" class="pull-left">
              <img
                class="media-object"
                src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/791e7e83d86ef0f28a60ec6eb61dbc784d0300f4/7-Figure8-1.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Feature Selective Anchor-Free Module for Single-Shot Object
                  Detection </strong
                ><br />
                Chenchen Zhu, <strong>Yihui He</strong>,
                <a target="_blank" href="http://www.cmu-biometrics.org/"
                  >Marios Savvides</a
                >, <strong>CVPR 2019</strong>
                <a target="_blank" href="https://arxiv.org/abs/1903.00621"
                  >[arXiv]</a
                >
              </p>
              <p class="abstract-text">
                We motivate and present feature selective anchor-free (FSAF)
                module, a simple and effective building block forsingle-shot
                object detectors. It can be plugged into single-shot detectors
                with feature pyramid structure. The FSAF module addresses two
                limitations brought up by the conventional anchor-based
                detection: 1) heuristic-guided feature selection; 2)
                overlap-based anchor sampling.
              </p>
            </div>
          </div>
          <div class="media">
            <a name="softer" class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/softer-NMS/master/demo/output/softer.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Bounding Box Regression with Uncertainty for Accurate Object
                  Detection </strong
                ><br />
                <strong>Yihui He</strong>, Chenchen Zhu, Jianren Wang,
                <a target="_blank" href="http://www.cmu-biometrics.org/"
                  >Marios Savvides</a
                >, Xiangyu Zhang, <strong>CVPR 2019</strong>
                <a target="_blank" href="https://arxiv.org/abs/1809.08545"
                  >[arXiv]</a
                >
                <a
                  target="_blank"
                  href="https://www.youtube.com/watch?v=bcGtNdTzdkc"
                  >[presentation]</a
                >
                <a target="_blank" href="https://github.com/yihui-he/KL-Loss"
                  >[code]</a
                >
              </p>
              <p class="abstract-text">
                We introduce a novel bounding box regression loss for learning
                bounding box transformation and localization variance together.
                The resulting localization variance is utilized in our new
                non-maximum suppression method to improve localization accuracy
                for object detection. On MS-COCO, we boost the AP of VGG-16
                faster R-CNN from 23.6% to 29.1% with a single model and nearly
                no additional computational overhead. More importantly, our
                method improves the AP of ResNet-50 FPN fast R-CNN from 36.8% to
                37.8%, which achieves state-of-the-art bounding box refinement
                result.
              </p>
            </div>
          </div>
          <div class="media">
            <a name="address" class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/yihui-he.github.io/master/assets/Screen%20Shot%202018-10-03%20at%2001.53.35.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  AddressNet: Shift-based Primitives for Efficient Convolutional
                  Neural Networks </strong
                ><br />
                <strong>Yihui He*</strong>, Xianggen Liu*, Huasong Zhong* and
                Yuchun Ma, <strong>WACV 2019</strong>
                <a target="_blank" href="https://arxiv.org/pdf/1809.08458.pdf"
                  >[arXiv]</a
                >
                <a
                  target="_blank"
                  href="https://www.youtube.com/watch?v=2Abi_razAXI"
                  >[presentation]</a
                >
                <a
                  target="_blank"
                  href="https://raw.githubusercontent.com/yihui-he/yihui-he.github.io/master/assets/WACV19-poster.png"
                  >[poster]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/yihui-he.github.io/blob/master/assets/AddressNet_%20Shift-based%20Primitives%20for%20Efficient%20CNN.pptx?raw=true"
                  >[slides]</a
                >
              </p>
              <p class="abstract-text">
                We propose a collection of three shift-based primitives for
                building efficient compact CNN-based networks. These three
                primitives (channel shift, address shift, shortcut shift) can
                reduce the inference time on GPU while maintains the prediction
                accuracy. These shift-based primitives only moves the pointer
                but avoids memory copy, thus very fast.
              </p>
            </div>
          </div>
          <div class="media">
            <a name="AMC" class="pull-left">
              <img
                class="media-object"
                src="./assets_files/mobile.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  AMC: AutoML for Model Compression and Acceleration on Mobile
                  Devices </strong
                ><br />
                <strong>Yihui He*</strong>, Ji Lin*, Zhijian Liu, Hanrui Wang,
                Li-Jia Li,
                <a target="_blank" href="http://songhan.mit.edu">Song Han</a>,
                <strong>ECCV 2018</strong>
                <a
                  target="_blank"
                  href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html"
                  >[PDF]</a
                >
                <a target="_blank" href="https://arxiv.org/abs/1802.03494"
                  >[arXiv]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/mit-han-lab/amc-compressed-models"
                  >[code]</a
                >
              </p>
              <p class="abstract-text">
                In this paper, we propose AMC: AutoML for Model Compression that
                leverage reinforcement learning to efficiently sample the design
                space and greatly improve the model compression quality.
              </p>
            </div>
          </div>
          <div class="media">
            <a name="cp" class="pull-left">
              <img
                class="media-object"
                src="./assets_files/ill-1.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Channel pruning for accelerating very deep neural networks </strong
                ><br />
                <strong>Yihui He</strong>, Xiangyu Zhang,
                <a target="_blank" href="http://jiansun.org/">Jian Sun</a>,
                <strong>ICCV 2017</strong>
                <a
                  target="_blank"
                  href="http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html"
                  >[PDF]</a
                >
                <a target="_blank" href="https://arxiv.org/abs/1707.06168"
                  >[arXiv]</a
                >
                <a
                  target="_blank"
                  href="https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png"
                  >[poster]</a
                >
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf"
                  >[slides]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/channel-pruning"
                  >[code]</a
                >
              </p>
              <p class="abstract-text">
                In this paper, we introduce a new channel pruning method to
                accelerate very deep convolutional neural networks.Given a
                trained CNN model, we propose an iterative two-step algorithm to
                effectively prune each layer, by a LASSO regression based
                channel selection and least square reconstruction. We further
                generalize this algorithm to multi-layer and multi-branch cases.
                Our method reduces the accumulated error and enhance the
                compatibility with various architectures. Our pruned VGG-16
                achieves the state-of-the-art results by 5x speed-up along with
                only 0.3% increase of error. More importantly, our method is
                able to accelerate modern networks like ResNet, Xception and
                suffers only 1.4%, 1.0% accuracy loss under 2x speed-up
                respectively, which is significant.
              </p>
            </div>
          </div>
          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="./assets_files/vehicle.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong
                  >Vehicle Traffic Driven Camera Placement for Better Metropolis
                  Security Surveillance</strong
                ><br />
                Xiaobo Ma*, <strong>Yihui He*</strong>, Xiapu Luo, Jianfeng Li,
                Xiaohong Guan , <strong>IEEE Intelligent System</strong>
                <a
                  target="_blank"
                  href="https://ieeexplore.ieee.org/abstract/document/8354911/"
                  >[PDF]</a
                >
                <a target="_blank" href="http://arxiv.org/abs/1705.08508"
                  >[arXiv]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/Vehicle-Traffic-Driven-Camera-Placement"
                  >[code]</a
                >
              </p>
              <p class="abstract-text">
                Security surveillance is one of the most important issues in
                smart cities, especially in an era of terrorism. Deploying a
                number of (video) cameras is a common approach for surveillance
                information retrieval. Given the never-ending power offered by
                vehicles to a metropolis, exploiting vehicle traffic to design
                camera placement strategies could potentially facilitate
                physicalworld security surveillance. We take the first step
                towards exploring the linkage between vehicle traffic and camera
                placement in favor of physical-world security surveillance from
                a network perspective.
              </p>
            </div>
          </div>
          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="./assets_files/filterwise.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Pruning Very Deep Neural Network Channels for Efficient
                  Inference </strong
                ><br />
                <strong>Yihui He</strong>, Xiangyu Zhang,
                <a target="_blank" href="http://jiansun.org/">Jian Sun</a> ,
                <i>TPAMI, Major Revision</i>
              </p>
              <p class="abstract-text">
                Channel Pruning is further expanded to Filterwise Pruning with
                rich experiements.
              </p>
            </div>
          </div>
          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="./assets_files/superres.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Single Image Super-resolution with a Parameter Economic
                  Residual-like Convolutional Neural Network </strong
                ><br />
                Yudong Liang, Ze Yang, Kai Zhang, <strong>Yihui He</strong>,
                Jinjun Wang, Nanning Zheng
                <a target="_blank" href="https://arxiv.org/abs/1703.08173"
                  >[arXiv]</a
                >
              </p>
              <p class="abstract-text">
                This paper aims to extend the merits of residual network, such
                as skip connection induced fast training, for a typical
                low-level vision problem, i.e., single image super-resolution.
                In general, the two main challenges of existing deep CNN for
                supper-resolution lie in the gradient exploding/vanishing
                problem and large amount of parameters or computational cost as
                CNN goes deeper. Correspondingly, the skip connections or
                identity mapping shortcuts are utilized to avoid gradient
                exploding/vanishing problem. To tackle with the second problem,
                a parameter economic CNN architecture which has carefully
                designed width, depth and skip connections was proposed.
                Different residual-like architectures for image superresolution
                has also been compared. Experimental results have demonstrated
                that the proposed CNN model can not only achieve
                state-of-the-art PSNR and SSIM results for single image
                super-resolution but also produce visually pleasant results.
              </p>
            </div>
          </div>

          <!--<div class="media">-->
          <!--<a class="pull-left" href="#top">-->
          <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
          <!--</a>-->
          <!--<div class="media-body">-->
          <!--<p class="media-heading">-->
          <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
          <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
          <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
          <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
          <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
          <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
          <!--</p>-->
          <!--<p class="abstract-text">-->
          <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
          <!--</p>-->
          <!--</div>-->
          <!--</div>-->

          <!--
             *** Projects ***
            -->
          <h3><a name="projects"></a> Projects</h3>
          Link to my
          <a target="_blank" href="https://github.com/yihui-he"
            >[github public projects]</a
          >

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/TSP-report/master/02132.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  An Empirical Analysis of Approximation Algorithms for the
                  Euclidean Traveling Salesman Problem
                </strong>
                <a target="_blank" href="https://arxiv.org/pdf/1705.09058.pdf"
                  >[PDF]</a
                >
                <a target="_blank" href="https://github.com/yihui-he/TSP"
                  >[Code]</a
                >
              </p>
              <p class="abstract-text">
                we perform an evaluation and analysis of cornerstone algorithms
                for the metric TSP. We evaluate greedy, 2-opt, and genetic
                algorithms. We use several datasets as input for the algorithms
                including a small dataset, a medium-sized dataset representing
                cities in the United States, and a synthetic dataset consisting
                of 200 cities to test algorithm scalability.
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  Estimated Depth Map Helps Image Classification
                </strong>
                <a target="_blank" href="https://arxiv.org/pdf/1709.07077.pdf"
                  >[PDF]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/Estimated-Depth-Map-Helps-Image-Classification"
                  >[Code]</a
                >
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/Estimated-Depth-Map-Helps-Image-Classification/blob/master/presentation/depth.pdf"
                  >[Slides]</a
                >
              </p>
              <p class="abstract-text">
                We consider image classification with estimated depth. This
                problem falls into the domain of transfer learning, since we are
                using a model trained on a set of depth images to generate depth
                maps (additional features) for use in another classification
                problem using another disjoint set of images. It's challenging
                as no direct depth information is provided. Though depth
                estimation has been well studied, none have attempted to aid
                image classification with estimated depth. Therefore, we present
                a way of transferring domain knowledge on depth estimation to a
                separate image classification task over a disjoint set of train,
                and test data. We build a RGBD dataset based on RGB dataset and
                do image classification on it. Then evaluation the performance
                of neural networks on the RGBD dataset compared to the RGB
                dataset. From our experiments, the benefit is significant with
                shallow and deep networks. It improves ResNet-20 by 0.55% and
                ResNet-56 by 0.53%.
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://www.kaggle.io/svf/310043/4567203286d71c8fd31cf12668a3ceac/__results___files/__results___5_0.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong> Medical Image Segmentation: A survey </strong>
                <a target="_blank" href="https://github.com/yihui-he/u-net"
                  >[Code]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md"
                  >[Summary]</a
                >
              </p>
              <p class="abstract-text">
                I evaluate DeepMask, Deeplab and MNC for medical image
                segmentation. I ranked
                <strong>19%</strong> on
                <a
                  target="_blank"
                  href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/leaderboard/private"
                  >Kaggle Ultrasound Nerve Segmentation</a
                >
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>
                  residual neural network with tensorflow on CIFAR-100
                </strong>
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/ResNet-tensorflow/blob/master/report/mp2_Yihui%20He.pdf"
                  >[PDF]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/ResNet-tensorflow"
                  >[Code]</a
                >
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf"
                  >[Slides]</a
                >
              </p>
              <p class="abstract-text">
                I reimplement 6/13/20 layers RNN in tensorflow from scratch, and
                test it’s result on CIFAR10/100.
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"
                width="96px"
                height="96px"
              />
            </a>

            <div class="media-body">
              <p class="media-heading">
                <strong
                  >Single Layer neural network with PCAwhitening Kmeans</strong
                >
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning/blob/master/report/mp1_Yihui%20He.pdf"
                  >[PDF]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning"
                  >[Code]</a
                >
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf"
                  >[Slides]</a
                >
              </p>
              <p class="abstract-text">
                We evaluate whether features extracted from the activation of a
                deep convolutional network trained in a fully supervised fashion
                on a large, fixed set of object recognition tasks can be
                re-purposed to novel generic tasks. We also released the
                software and pre-trained network to do large-scale image
                classification.
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png"
                width="96px"
                height="96px"
              />
            </a>

            <div class="media-body">
              <p class="media-heading">
                <strong>Objects Detection with YOLO on Artwork Dataset</strong>
                <a
                  target="_blank"
                  href="http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf"
                  >[PDF]</a
                >
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset"
                  >[Code]</a
                >
              </p>
              <p class="abstract-text">
                I design a small object detection network, which is simplified
                from YOLO(You Only Look Once) network. It's trained on PASCAL
                VOC. I evaluate it on an artwork dataset(Picasso dataset). With
                the best parameters, I got 40% precision and 35% recall.
              </p>
            </div>
          </div>

          <div class="media">
            <a class="pull-left">
              <img
                class="media-object"
                src="./assets_files/shuttle.png"
                width="96px"
                height="96px"
              />
            </a>
            <div class="media-body">
              <p class="media-heading">
                <strong>shuttlecock detection and tracking</strong>
                <a
                  target="_blank"
                  href="https://github.com/yihui-he/Badminton-Robot"
                  >[Code]</a
                >
              </p>
              <p class="abstract-text">
                With guassian mixture model, I extract shuttlecock proposals.
                Then I use Partical filter to refine proposals. From multi view
                cameras, I employed structure from motion to predict its 3D
                location. Combined with Physics laws, landing location
                prediction accuracy is around 5 cm. (This system works on
                embeded linux with openCV)
              </p>
            </div>
          </div>
          <!-- Footer
            ================================================== -->
          <hr />
          <footer class="footer">
            <div class="hidden-phone">
              <!-- <h3 class="text-center"><a name="wall"></a><strong>works</strong></h3>-->
              <section id="photos">
                <img
                  src="https://raw.githubusercontent.com/yihui-he/lip-tracking-with-snake-active-contour-and-particle-filter/master/pic.png"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/Edge-detection-with-zero-crossing/master/lena_1.bmp"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/3D-reconstruction/master/result/selfff.png"
                />
                <img src="./assets_files/cs188.png" />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png"
                />
                <img src="./assets_files/vehicle.png" />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"
                />
                <img src="./assets_files/shuttle.png" />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/gaoxin.jpg"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/artwork.jpg"
                />
                <img
                  src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/bop.jpg"
                />
                <img src="./assets_files/ocsi.png" />
              </section>

              <a target="_blank" href="https://github.com/yihui-he/panorama"
                ><img
                  src="https://github.com/yihui-he/panorama/blob/master/results/yellowstone5.jpg?raw=true"
              /></a>
              <hr />
            </div>
            <div class="row">
              <div class="span12">
                <p>
                  modified from
                  <a target="_blank" href="http://daggerfs.com/"
                    >© Yangqing Jia 2013</a
                  >
                </p>
              </div>
            </div>
          </footer>
        </div>
      </div>
    </div>
  </body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
