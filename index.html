<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yihui He, CMU grad student focused on Computer vision & Deep Learning</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    <link rel="icon" href="./assets_files/favicon.ico">
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#research">Research</a>-->
    <a href="#publications">Publications</a>
    <a href="#projects">Projects</a>
    <!--<a href="#teaching">Teaching</a>-->
    <a target="_blank"
       href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8">CV</a>
</div>

<body>

<div class="container span3 hidden-phone">
    <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br>
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="#top">About</a></li>
            <!--<li><a href="#research">Research</a></li>-->
            <li><a href="#publications">Publications</a></li>
            <li><a href="#projects">Projects</a></li>
            <!--<li><a href="#teaching">Teaching</a></li>-->
            <li><a target="_blank"
                   href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8">CV</a>
            </li>
        </ul>
        <hr class="hidden-phone">
        <div class="text-center hidden-phone">
            <img src="assets_files/me.png" alt="photo" class="logo-image">
            <br><br>
            yihuihe.yh AT gmail.com <br>
        </div>

        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Yihui He (何宜晖)
            </h3>
            <h5>
                yihuihe.yh AT gmail DOT com
            / <a target="_blank"
                 href="https://scholar.google.com/citations?user=2yAMJ1YAAAAJ&hl=en">Google Scholar</a>
            / <a target="_blank" 
                 href="https://github.com/yihui-he">GitHub</a>
            / <a target="_blank"
                 href="https://drive.google.com/open?id=16ZSY62e2LgWYXuYuX4GVbbJ3OnU9bwl8">CV</a>
            / <mark>actively looking for job</mark>
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="http://daggerfs.com/index.html#">
                <img class="media-object" src="assets_files/me.png" width="96px" style="margin: 0px 10px">
            </a>
            <p>
                I'm a CMU master student, with my interest focus on Computer Vision and Deep Learning. 
                At CMU, my capstone project is on multi-view pose estimation, with professor <a target="_blank" href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>. I interned at Facebook AI Research mentored by <a target="_blank" href="https://research.fb.com/people/goyal-priya/">Priya Goyal</a> and <a target="_blank" href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>.      
            </p>
            <p>
                I have a track record of contributing to CNN efficient inference during my undergrad study. 
            Particularly, I designed <a href="#cp">channel pruning</a> to effectively prune channels. 
            I further proposed <a href="#AMC">AMC</a> to sample the design space of channel pruning via reinforcement learning, which greatly improved the performance.
             </p>  
            <p>I served as a reviewer for ECCV'20, ICML'20, CVPR'20, ICLR'20, ICCV'19, CVPR'19, ICLR'19, NIPS'18, Pattern Recognition Letters, TIP and IJCV. </p>

            <!--
             *** Research ***
            -->
            <!--<h3>-->
            <!--<a name="research"></a> Research-->
            <!--</h3>-->
            <!--<p>-->
            <!--My current research topics include:-->
            <!--</p><ul>-->
            <!--<li> Learning better structures for image feature extraction.-->
            <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
            <!--</li><li> Making large-scale vision feasible and affordable.-->
            <!--</li></ul>-->
            <!--<p></p>-->
            <!--<p> (Most recent publications to be added) </p>-->


            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Publications
            </h3>
            <!--
            <div class="media">
                <a name="pts" class="pull-left">
                    <img class="media-object" src="./assets/pts.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Prediction-Tracking-Segmentation
                     </strong><br>
                        Jianren Wang, <strong>Yihui He</strong>, Xiaobo Wang, Xinjia Yu, Xia Chen
                        <a target="_blank"
                           href="https://arxiv.org/abs/1904.03280">[arXiv]</a>
                    </p>
                    <p class="abstract-text">
                        We motivate and present feature selective anchor-free(FSAF) module, a simple and effective building block forsingle-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. 
                    </p>
                </div>
            </div>
-->
                  <div class="media">
                <a name="fsaf" class="pull-left">
                    <img class="media-object" src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/791e7e83d86ef0f28a60ec6eb61dbc784d0300f4/7-Figure8-1.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Feature Selective Anchor-Free Module for Single-Shot Object Detection
                     </strong><br>
                        Chenchen Zhu, <strong>Yihui He</strong>, <a target="_blank" href="http://www.cmu-biometrics.org/">Marios Savvides</a>, <strong>CVPR 2019</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/1903.00621">[arXiv]</a>  
                        
                        
                    </p>
                    <p class="abstract-text">
                        We motivate and present feature selective anchor-free (FSAF) module, a simple and effective building block forsingle-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. 
                    </p>
                </div>
            </div>
         <div class="media">
                <a name="softer" class="pull-left">
                    <img class="media-object" src="https://raw.githubusercontent.com/yihui-he/softer-NMS/master/demo/output/softer.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Bounding Box Regression with Uncertainty for Accurate Object Detection
                     </strong><br>
                        <strong>Yihui He</strong>, Chenchen Zhu, Jianren Wang, <a target="_blank" href="http://www.cmu-biometrics.org/">Marios Savvides</a>, Xiangyu Zhang, <strong>CVPR 2019</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/1809.08545">[arXiv]</a>  
                        <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>                          
                        <a target="_blank"
                           href="https://github.com/yihui-he/KL-Loss">[code]</a>  
                        
                        
                    </p>
                    <p class="abstract-text">
                        We introduce a novel bounding box regression loss for learning bounding box transformation and localization variance together. The resulting localization variance is utilized in our new non-maximum suppression method to improve localization accuracy for object detection. On MS-COCO, we boost the AP of VGG-16 faster R-CNN from 23.6% to 29.1% with a single model and nearly no additional computational overhead. More importantly, our method improves the AP of ResNet-50 FPN fast R-CNN from 36.8% to 37.8%, which achieves state-of-the-art bounding box refinement result.
                    </p>
                </div>
            </div>
         <div class="media">
                <a name="address" class="pull-left">
                    <img class="media-object" src="https://raw.githubusercontent.com/yihui-he/yihui-he.github.io/master/assets/Screen%20Shot%202018-10-03%20at%2001.53.35.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             AddressNet: Shift-based Primitives for Efficient Convolutional Neural Networks
                     </strong><br>
                            <strong>Yihui He*</strong>, Xianggen Liu*, Huasong Zhong* and Yuchun Ma, <strong>WACV 2019</strong>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/1809.08458.pdf">[arXiv]</a>  
                        <a target="_blank"
                           href="https://www.youtube.com/watch?v=2Abi_razAXI">[presentation]</a>  
                        <a target="_blank"
                           href="https://raw.githubusercontent.com/yihui-he/yihui-he.github.io/master/assets/WACV19-poster.png">[poster]</a> 
                        <a target="_blank"
                           href="https://github.com/yihui-he/yihui-he.github.io/blob/master/assets/AddressNet_%20Shift-based%20Primitives%20for%20Efficient%20CNN.pptx?raw=true">[slides]</a> 
                        
                    </p>
                    <p class="abstract-text">
                       We propose a collection of three shift-based primitives for building efficient compact CNN-based networks. These three primitives (channel shift, address shift, shortcut shift) can reduce the inference time on GPU while maintains the prediction accuracy. These shift-based primitives only moves the pointer but avoids memory copy, thus very fast.
                    </p>
                </div>
            </div> 
        <div class="media">
                <a name="AMC" class="pull-left">
                    <img class="media-object" src="./assets_files/mobile.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             AMC: AutoML for Model Compression and Acceleration on Mobile Devices
                     </strong><br>
                        <strong>Yihui He*</strong>, Ji Lin*, Zhijian Liu, Hanrui Wang, Li-Jia Li, <a target="_blank" href="http://songhan.mit.edu">Song Han</a>, <strong>ECCV 2018</strong>
                        <a target="_blank"
                           href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html">[PDF]</a>  
                         <a target="_blank"
                           href="https://arxiv.org/abs/1802.03494">[arXiv]</a> 
                        <a target="_blank"
                           href="https://github.com/mit-han-lab/amc-compressed-models">[code]</a> 
                        
                        
                    </p>
                    <p class="abstract-text">
                        In this paper, we propose AMC: AutoML for Model Compression that leverage reinforcement learning to efficiently sample the design space and greatly improve the model compression quality.
                    </p>
                </div>
            </div>              
        <div class="media">
                <a name="cp" class="pull-left">
                    <img class="media-object" src="./assets_files/ill-1.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Channel pruning for accelerating very deep neural networks
                     </strong><br>
                        <strong>Yihui He</strong>, Xiangyu Zhang, <a target="_blank" href="http://jiansun.org/">Jian Sun</a>, <strong>ICCV 2017</strong>
                        <a target="_blank"
                           href="http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html">[PDF]</a>  
                        <a target="_blank"
                           href="https://arxiv.org/abs/1707.06168">[arXiv]</a>                          
                        <a target="_blank"
                           href="https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png">[poster]</a>     
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf">[slides]</a>                             
                        <a target="_blank"
                           href="https://github.com/yihui-he/channel-pruning">[code]</a>       
                       
                    </p>
                    <p class="abstract-text">
                        In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.
                    </p>
                </div>
            </div>   
                <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/vehicle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>Vehicle Traffic Driven Camera Placement for Better Metropolis Security
                            Surveillance</strong><br>
                        Xiaobo Ma*, <strong>Yihui He*</strong>, Xiapu Luo, Jianfeng Li, Xiaohong Guan
                        , <strong>IEEE Intelligent System</strong>
                        <a target="_blank"
                           href="https://ieeexplore.ieee.org/abstract/document/8354911/">[PDF]</a>
                        <a target="_blank"
                           href="http://arxiv.org/abs/1705.08508">[arXiv]</a>
                        <a target="_blank"
                           href="https://github.com/yihui-he/Vehicle-Traffic-Driven-Camera-Placement">[code]</a>                        
                    </p>
                    <p class="abstract-text">
                        Security surveillance is one of the most important issues in smart cities, especially in an era
                        of
                        terrorism. Deploying a number of (video) cameras is a
                        common approach for surveillance information retrieval.
                        Given the never-ending power offered by vehicles to a
                        metropolis, exploiting vehicle traffic to design camera
                        placement strategies could potentially facilitate physicalworld security surveillance. We take
                        the first step towards
                        exploring the linkage between vehicle traffic and camera
                        placement in favor of physical-world security surveillance
                        from a network perspective.
                    </p>
                </div>
            </div>
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/filterwise.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Pruning Very Deep Neural Network Channels for Efficient Inference
                     </strong><br>
                        <strong>Yihui He</strong>, Xiangyu Zhang, <a target="_blank" href="http://jiansun.org/">Jian Sun</a>
                        , <i>TPAMI, Major Revision</i>
                    </p>
                    <p class="abstract-text">
                        Channel Pruning is further expanded to Filterwise Pruning with rich experiements.
                    </p>
                </div>
            </div>
            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/superres.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Single Image Super-resolution with a Parameter Economic Residual-like Convolutional Neural Network
                        </strong><br>
                        Yudong Liang, Ze Yang, Kai Zhang, <strong>Yihui He</strong>, Jinjun Wang, Nanning Zheng
                        <a target="_blank"
                           href="https://arxiv.org/abs/1703.08173">[arXiv]</a>
                    </p>
                    <p class="abstract-text">
                        This paper aims to extend the merits of residual network, such as skip
                        connection induced fast training, for a typical low-level vision problem,
                        i.e., single image super-resolution. In general, the two main challenges
                        of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing
                        problem and large amount of parameters or computational
                        cost as CNN goes deeper. Correspondingly, the skip connections or identity
                        mapping shortcuts are utilized to avoid gradient exploding/vanishing
                        problem. To tackle with the second problem, a parameter economic CNN
                        architecture which has carefully designed width, depth and skip connections
                        was proposed. Different residual-like architectures for image superresolution
                        has also been compared. Experimental results have demonstrated
                        that the proposed CNN model can not only achieve state-of-the-art
                        PSNR and SSIM results for single image super-resolution but also produce
                        visually pleasant results.
                    </p>
                </div>
            </div>        


            <!--<div class="media">-->
            <!--<a class="pull-left" href="#top">-->
            <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
            <!--</a>-->
            <!--<div class="media-body">-->
            <!--<p class="media-heading">-->
            <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
            <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
            <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
            <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
            <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
            <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
            <!--</p>-->
            <!--<p class="abstract-text">-->
            <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
            <!--</p>-->
            <!--</div>-->
            <!--</div>-->

            <!--
             *** Projects ***
            -->
            <h3>
                <a name="projects"></a> Projects
            </h3>
            Link to my <a target="_blank" href="https://github.com/yihui-he">[github public projects]</a>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/TSP-report/master/02132.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem
                        </strong>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/1705.09058.pdf">[PDF]</a>
                        <a target="_blank"
                           href="https://github.com/yihui-he/TSP">[Code]</a>
                    </p>
                    <p class="abstract-text">
we perform an evaluation and analysis of cornerstone algorithms for the metric TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use
several datasets as input for the algorithms including a small dataset, a medium-sized dataset representing
cities in the United States, and a synthetic dataset consisting of 200 cities to test algorithm scalability.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Estimated Depth Map Helps Image Classification
                        </strong>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/1709.07077.pdf">[PDF]</a>
                        <a target="_blank" href="https://github.com/yihui-he/Estimated-Depth-Map-Helps-Image-Classification">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/Estimated-Depth-Map-Helps-Image-Classification/blob/master/presentation/depth.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        We consider image classification with estimated depth. This problem falls into the domain of transfer learning, since we are using a model trained on a set of depth images to generate depth maps (additional features) for use in another classification problem using another disjoint set of images. It's challenging as no direct depth information is provided. Though depth estimation has been well studied, none have attempted to aid image classification with estimated depth. Therefore, we present a way of transferring domain knowledge on depth estimation to a separate image classification task over a disjoint set of train, and test data. We build a RGBD dataset based on RGB dataset and do image classification on it. Then evaluation the performance of neural networks on the RGBD dataset compared to the RGB dataset. From our experiments, the benefit is significant with shallow and deep networks. It improves ResNet-20 by 0.55% and ResNet-56 by 0.53%.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://www.kaggle.io/svf/310043/4567203286d71c8fd31cf12668a3ceac/__results___files/__results___5_0.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Medical Image Segmentation: A survey
                        </strong>
                        <a target="_blank" href="https://github.com/yihui-he/u-net">[Code]</a>
                        <a target="_blank"
                           href="https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md">[Summary]</a>
                    </p>
                    <p class="abstract-text">
                        I evaluate DeepMask, Deeplab and MNC for medical image segmentation. I ranked
                        <strong>19%</strong> on <a target="_blank"
                                                   href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/leaderboard/private">Kaggle
                        Ultrasound Nerve Segmentation</a>
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            residual neural network with tensorflow on CIFAR-100 
                        </strong>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/ResNet-tensorflow/blob/master/report/mp2_Yihui%20He.pdf">[PDF]</a>
                        <a target="_blank" href="https://github.com/yihui-he/ResNet-tensorflow">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        I reimplement 6/13/20 layers RNN in tensorflow from scratch, and test it’s result on
                        CIFAR10/100.
                    </p>
                </div>
            </div>


            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"
                         width="96px" height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong>Single Layer neural network with PCAwhitening Kmeans</strong> 
                        <a target="_blank" href="http://nbviewer.jupyter.org/github/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning/blob/master/report/mp1_Yihui%20He.pdf">[PDF]</a>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        We evaluate whether features extracted from the activation of a deep convolutional network
                        trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be
                        re-purposed to novel generic tasks. We also released the software and pre-trained network to do
                        large-scale image classification.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png" width="96px"
                         height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong>Objects Detection with YOLO on Artwork Dataset</strong> 
                        <a target="_blank" href="http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf">[PDF]</a>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        I design a small object detection network, which is simplified from YOLO(You Only Look Once)
                        network. It's trained on PASCAL VOC. I evaluate it on an artwork dataset(Picasso dataset). With
                        the best parameters, I got 40% precision and 35% recall.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/shuttle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>shuttlecock detection and tracking</strong>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/Badminton-Robot">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        With guassian mixture model, I extract shuttlecock proposals. Then I use Partical filter to
                        refine proposals. From multi view cameras, I employed
                        structure from motion to predict its 3D location. Combined with Physics laws, landing location
                        prediction accuracy is around 5 cm. (This system
                        works on embeded linux with openCV)
                    </p>
                </div>
            </div>
            <!-- Footer
            ================================================== -->
            <hr>
            <footer class="footer">
                <div class='hidden-phone'>
                <!-- <h3 class="text-center"><a name="wall"></a><strong>works</strong></h3>-->
                <section id="photos">
                    <img src="https://raw.githubusercontent.com/yihui-he/lip-tracking-with-snake-active-contour-and-particle-filter/master/pic.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Edge-detection-with-zero-crossing/master/lena_1.bmp"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/3D-reconstruction/master/result/selfff.png"/>
                    <img src="./assets_files/cs188.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png"/>
                    <img src="./assets_files/vehicle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"/>
                    <img src="./assets_files/shuttle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/gaoxin.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/artwork.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/bop.jpg"/>
                    <img src="./assets_files/ocsi.png"/>
                </section>
                
                <a target="_blank" href="https://github.com/yihui-he/panorama"><img
                        src="https://github.com/yihui-he/panorama/blob/master/results/yellowstone5.jpg?raw=true"></a>
                <hr>
                </div>
                <div class="row">
                    <div class="span12">
                        <p>
                            modified from <a target="_blank" href="http://daggerfs.com/">© Yangqing Jia 2013</a>
                        </p>
                    </div>
                </div>

            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
